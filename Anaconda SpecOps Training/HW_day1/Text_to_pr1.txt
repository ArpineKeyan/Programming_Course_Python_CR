The BERT model was proposed in BERT: 
Pre-training of Deep Bidirectional Transformers for Language Understanding by Jacob Devlin, Ming-Wei Chang, 
Kenton Lee and Kristina Toutanova. Itâ€™s a bidirectional transformer pretrained using a combination of masked 
language modeling objective and next sentence prediction on a large corpus comprising the Toronto Book Corpus 
and Wikipedia.
(vocab_size = 30522
hidden_size = 768
num_hidden_layers = 12
num_attention_heads = 12
intermediate_size = 3072
hidden_act = 'gelu'
hidden_dropout_prob = 0.1
attention_probs_dropout_prob = 0.1
max_position_embeddings = 512
type_vocab_size = 2
initializer_range = 0.02
layer_norm_eps = 1e-12
pad_token_id = 0
position_embedding_type = 'absolute'
use_cache = True
classifier_dropout = None**kwargs )
